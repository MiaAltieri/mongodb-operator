[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  shard-one/0 [executing] active: Primary
  shard-one/1 [executing] active: 
  shard-two/0 [executing] active: Primary
  config-server/0 [executing] active: 
  config-server/1 [executing] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] active: Primary
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] maintenance: backup started/running, backup id: '2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server/1 [idle] active: Primary
[32mINFO    [0m juju.model:__init__.py:2301 Deploying local:jammy/mongodb-3
[32mINFO    [0m juju.model:__init__.py:2301 Deploying local:jammy/mongodb-4
[32mINFO    [0m juju.model:__init__.py:2301 Deploying local:jammy/mongodb-5
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  s3-integrator/0 [idle] active: 
  config-server-new/0 [allocating] waiting: waiting for machine
  config-server-new/1 [allocating] waiting: waiting for machine
  shard-one-new/0 [allocating] waiting: waiting for machine
  shard-one-new/1 [allocating] waiting: waiting for machine
  shard-two-new/0 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [allocating] waiting: waiting for machine
  config-server-new/1 [allocating] waiting: waiting for machine
  shard-one-new/0 [allocating] waiting: waiting for machine
  shard-one-new/1 [allocating] waiting: waiting for machine
  shard-two-new/0 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [allocating] waiting: waiting for machine
  config-server-new/1 [allocating] waiting: waiting for machine
  shard-one-new/0 [allocating] waiting: waiting for machine
  shard-one-new/1 [allocating] waiting: waiting for machine
  shard-two-new/0 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [allocating] waiting: waiting for machine
  config-server-new/1 [allocating] waiting: waiting for machine
  shard-one-new/0 [allocating] waiting: waiting for machine
  shard-one-new/1 [allocating] waiting: waiting for machine
  shard-two-new/0 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [executing] maintenance: Installed MongoDB
  config-server-new/1 [executing] maintenance: Installed MongoDB
  shard-one-new/0 [executing] maintenance: installing MongoDB
  shard-one-new/1 [executing] maintenance: installing MongoDB
  shard-two-new/0 [executing] maintenance: installing charm software
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [executing] active: 
  config-server-new/1 [executing] active: 
  shard-one-new/0 [executing] active: 
  shard-one-new/1 [executing] active: 
  shard-two-new/0 [executing] maintenance: starting MongoDB
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [idle] active: 
  config-server-new/1 [idle] active: 
  shard-one-new/0 [idle] active: 
  shard-two-new/0 [idle] active: 
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [idle] active: 
  config-server-new/1 [idle] active: 
  shard-one-new/0 [executing] active: 
  shard-one-new/1 [executing] active: 
  shard-two-new/0 [executing] active: 
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [executing] active: 
  config-server-new/1 [executing] active: 
  shard-one-new/0 [executing] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-one-new/1 [executing] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-two-new/0 [idle] maintenance: Adding shard to config-server
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [executing] active: 
  config-server-new/1 [executing] waiting: waiting to sync s3 configurations.
  shard-one-new/0 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-one-new/1 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-two-new/0 [idle] maintenance: Adding shard to config-server
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [executing] active: 
  config-server-new/1 [executing] waiting: waiting to sync s3 configurations.
  shard-one-new/0 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-one-new/1 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-two-new/0 [idle] maintenance: Adding shard to config-server
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [idle] active: 
  config-server-new/1 [executing] maintenance: Adding shard shard-one-new to config-server
  shard-one-new/0 [executing] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-one-new/1 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-two-new/0 [idle] maintenance: Adding shard to config-server
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  shard-one-new/0 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-one-new/1 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-two-new/0 [idle] maintenance: Adding shard to config-server
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  shard-one-new/0 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-one-new/1 [idle] blocked: Charm revision (1+dfeb7f0-dirty-locally built) is not up-to date with config-server.
  shard-two-new/0 [idle] maintenance: Adding shard to config-server
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [executing] active: 
  config-server-new/1 [executing] active: Primary
  shard-one-new/0 [idle] active: Primary
  shard-one-new/1 [idle] active: 
  shard-two-new/0 [idle] active: Primary
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  shard-one-new/0 [executing] active: Primary
  shard-one-new/1 [executing] active: 
  shard-two-new/0 [executing] active: Primary
  config-server-new/0 [executing] active: 
  config-server-new/1 [executing] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] waiting: Waiting to sync passwords across the cluster
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] active: Primary
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/0 [idle] active: 
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] maintenance: restore started/running, backup id:'2025-02-16T02:10:28Z'
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  config-server-new/1 [idle] active: Primary
[32mINFO    [0m pytest_operator.plugin:plugin.py:903 Model status:

Model  Controller           Cloud/Region         Version  SLA          Timestamp
test   localhost-localhost  localhost/localhost  3.6.1    unsupported  02:35:40Z

App                Version  Status  Scale  Charm          Channel      Rev  Exposed  Message
config-server-new  6.0.6    active      2  mongodb                       3  no       
s3-integrator               active      1  s3-integrator  latest/edge  129  no       
shard-one-new      6.0.6    active      2  mongodb                       4  no       
shard-two-new      6.0.6    active      1  mongodb                       5  no       

Unit                  Workload  Agent  Machine  Public address  Ports            Message
config-server-new/0   active    idle   6        10.189.170.230  27017-27018/tcp  
config-server-new/1*  active    idle   7        10.189.170.69   27017-27018/tcp  Primary
s3-integrator/0*      active    idle   5        10.189.170.159                   
shard-one-new/0*      active    idle   8        10.189.170.85   27017/tcp        Primary
shard-one-new/1       active    idle   9        10.189.170.189  27017/tcp        
shard-two-new/0*      active    idle   10       10.189.170.75   27017/tcp        Primary

Machine  State    Address         Inst id         Base          AZ  Message
5        started  10.189.170.159  juju-c34844-5   ubuntu@22.04      Running
6        started  10.189.170.230  juju-c34844-6   ubuntu@22.04      Running
7        started  10.189.170.69   juju-c34844-7   ubuntu@22.04      Running
8        started  10.189.170.85   juju-c34844-8   ubuntu@22.04      Running
9        started  10.189.170.189  juju-c34844-9   ubuntu@22.04      Running
10       started  10.189.170.75   juju-c34844-10  ubuntu@22.04      Running

[32mINFO    [0m pytest_operator.plugin:plugin.py:909 Juju error logs:

machine-1: 01:32:19 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-1: 01:32:19 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-config-server-1: 01:32:19 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-0: 01:32:21 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-0: 01:32:21 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-config-server-0: 01:32:21 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-2: 01:32:35 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-3: 01:32:35 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-3: 01:32:35 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-2: 01:32:35 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-shard-one-1: 01:32:35 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
unit-shard-one-0: 01:32:35 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
unit-config-server-0: 01:32:52 ERROR unit.config-server/0.juju-log Unable to set params: ['vm.max_map_count']
unit-config-server-0: 01:32:52 ERROR unit.config-server/0.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
machine-4: 01:32:52 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-4: 01:32:52 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-5: 01:32:52 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-5: 01:32:52 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-shard-two-0: 01:32:52 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
unit-s3-integrator-0: 01:32:52 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
unit-config-server-1: 01:32:53 ERROR unit.config-server/1.juju-log Unable to set params: ['vm.max_map_count']
unit-config-server-1: 01:32:53 ERROR unit.config-server/1.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-config-server-0: 01:33:00 ERROR unit.config-server/0.juju-log This operation (update_relation_data()) can only be performed by the leader unit
unit-shard-one-0: 01:33:03 ERROR unit.shard-one/0.juju-log Unable to set params: ['vm.max_map_count']
unit-shard-one-0: 01:33:03 ERROR unit.shard-one/0.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-shard-one-1: 01:33:04 ERROR unit.shard-one/1.juju-log Unable to set params: ['vm.max_map_count']
unit-shard-one-1: 01:33:04 ERROR unit.shard-one/1.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-shard-two-0: 01:33:19 ERROR unit.shard-two/0.juju-log Unable to set params: ['vm.max_map_count']
unit-shard-two-0: 01:33:19 ERROR unit.shard-two/0.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-shard-one-1: 01:34:54 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140bc60da0988982e9fee, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: RSSecondary, rtt: 0.0009979080023185816>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0008223490003729239>]>
unit-shard-one-1: 01:34:58 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140c160da0988982e9fef, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: RSSecondary, rtt: 0.0009886000007099938>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0010275530003127642>]>
unit-shard-one-1: 01:35:02 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140c560da0988982e9ff0, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: Unknown, rtt: None, error=NotPrimaryError("The server is in quiesce mode and will shut down, full error: {'topologyVersion': {'processId': ObjectId('67b140bcd66570b0bc76bc57'), 'counter': 4}, 'ok': 0.0, 'errmsg': 'The server is in quiesce mode and will shut down', 'code': 91, 'codeName': 'ShutdownInProgress', 'remainingQuiesceTimeMillis': 11825}")>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.000814642000477761>]>
unit-shard-one-1: 01:35:06 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140c960da0988982e9ff1, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: Unknown, rtt: None, error=NotPrimaryError("The server is in quiesce mode and will shut down, full error: {'topologyVersion': {'processId': ObjectId('67b140bcd66570b0bc76bc57'), 'counter': 4}, 'ok': 0.0, 'errmsg': 'The server is in quiesce mode and will shut down', 'code': 91, 'codeName': 'ShutdownInProgress', 'remainingQuiesceTimeMillis': 7627}")>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0010913639998761937>]>
unit-shard-one-1: 01:35:10 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140cd60da0988982e9ff2, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: Unknown, rtt: None, error=NotPrimaryError("The server is in quiesce mode and will shut down, full error: {'topologyVersion': {'processId': ObjectId('67b140bcd66570b0bc76bc57'), 'counter': 4}, 'ok': 0.0, 'errmsg': 'The server is in quiesce mode and will shut down', 'code': 91, 'codeName': 'ShutdownInProgress', 'remainingQuiesceTimeMillis': 3457}")>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0006393319999915548>]>
unit-shard-one-1: 01:35:14 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140d160da0988982e9ff3, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0007852679991628975>]>
unit-shard-one-1: 01:35:19 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140d660da0988982e9ff4, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: RSSecondary, rtt: 0.000991414999589324>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0009416220018465538>]>
unit-shard-one-1: 01:35:23 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140da60da0988982e9ff5, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: RSSecondary, rtt: 0.0011603140010265633>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0010133670002687722>]>
unit-shard-one-1: 01:35:55 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140fa812e7179fbe83e6c, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: Unknown, rtt: None, error=NotPrimaryError("The server is in quiesce mode and will shut down, full error: {'topologyVersion': {'processId': ObjectId('67b140d3bef8a9c98b9043d6'), 'counter': 6}, 'ok': 0.0, 'errmsg': 'The server is in quiesce mode and will shut down', 'code': 91, 'codeName': 'ShutdownInProgress', 'remainingQuiesceTimeMillis': 3739, 'lastCommittedOpTime': Timestamp(1739669730, 7), '$clusterTime': {'clusterTime': Timestamp(1739669751, 1), 'signature': {'hash': b'p\\xc2+!]\\x1do\\x9e\\x91\\x97\\xbe\\x8d\\x00\\x00\\xe7\\x9d\\x85-\\xc9\\xee', 'keyId': 7471823994895728660}}, 'operationTime': Timestamp(1739669730, 7)}")>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0008094330005405936>]>
unit-shard-one-1: 01:35:59 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b140fe812e7179fbe83e6d, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('10.189.170.190:27017: [Errno 104] Connection reset by peer (configured timeouts: socketTimeoutMS: 2000.0ms, connectTimeoutMS: 2000.0ms)')>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0011351360008120537>]>
unit-shard-one-1: 01:36:04 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14102812e7179fbe83e6e, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: RSSecondary, rtt: 0.000995873000647407>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0009419320012966637>]>
unit-shard-one-1: 01:36:08 ERROR unit.shard-one/1.juju-log sharding:8: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14107812e7179fbe83e6f, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.190', 27017) server_type: RSSecondary, rtt: 0.0011295449985482264>, <ServerDescription ('10.189.170.81', 27017) server_type: RSSecondary, rtt: 0.0010738809978647623>]>
unit-shard-one-1: 01:38:59 ERROR unit.shard-one/1.juju-log Backup failed: Relation with s3-integrator charm missing, cannot restore from a backup.
unit-shard-one-1: 01:39:02 ERROR unit.shard-one/1.juju-log List-backups failed: Relation with s3-integrator charm missing, cannot restore from a backup.
unit-shard-one-1: 01:39:05 ERROR unit.shard-one/1.juju-log Restore failed: Relation with s3-integrator charm missing, cannot restore from a backup.
unit-config-server-1: 01:44:11 ERROR unit.config-server/1.juju-log cmd failed - cmd=['/snap/bin/charmed-mongodb.pbm', 'status', '-o', 'json'], stdout={"Error":"get status of cluster: connect to `shard-one` [shard-one/10.189.170.190:27017,10.189.170.81:27017]: ping: connection() error occurred during connection handshake: auth error: unable to authenticate using mechanism \"SCRAM-SHA-256\": (AuthenticationFailed) Authentication failed."}
, stderr=cmd_run.go:1276: WARNING: cannot create user data directory: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH
cmd_run.go:1281: WARNING: cannot copy user Xauthority file: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH

unit-config-server-1: 01:44:11 ERROR unit.config-server/1.juju-log Failed to get pbm status: cmd failed (1) - cmd=['/snap/bin/charmed-mongodb.pbm', 'status', '-o', 'json'], stdout={"Error":"get status of cluster: connect to `shard-one` [shard-one/10.189.170.190:27017,10.189.170.81:27017]: ping: connection() error occurred during connection handshake: auth error: unable to authenticate using mechanism \"SCRAM-SHA-256\": (AuthenticationFailed) Authentication failed."}
, stderr=cmd_run.go:1276: WARNING: cannot create user data directory: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH
cmd_run.go:1281: WARNING: cannot copy user Xauthority file: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH

unit-config-server-1: 02:16:37 ERROR unit.config-server/1.juju-log config-server:9: cannot remove shard shard-two from cluster, another shard is draining
unit-config-server-1: 02:16:37 ERROR unit.config-server/1.juju-log config-server:9: Deferring _on_relation_event for shards interface since: error=cannot remove shard shard-two from cluster, another shard is draining
machine-7: 02:20:48 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-7: 02:20:48 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-config-server-new-1: 02:20:48 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-6: 02:20:52 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-6: 02:20:52 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-config-server-new-0: 02:20:52 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-8: 02:20:58 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-8: 02:20:58 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-shard-one-new-0: 02:20:59 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-9: 02:20:59 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-9: 02:20:59 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-shard-one-new-1: 02:20:59 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-10: 02:21:05 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-10: 02:21:05 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-shard-two-new-0: 02:21:05 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
unit-config-server-new-1: 02:21:11 ERROR unit.config-server-new/1.juju-log Unable to set params: ['vm.max_map_count']
unit-config-server-new-1: 02:21:11 ERROR unit.config-server-new/1.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-config-server-new-0: 02:21:15 ERROR unit.config-server-new/0.juju-log Unable to set params: ['vm.max_map_count']
unit-config-server-new-0: 02:21:15 ERROR unit.config-server-new/0.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-shard-one-new-1: 02:21:24 ERROR unit.shard-one-new/1.juju-log Unable to set params: ['vm.max_map_count']
unit-shard-one-new-1: 02:21:24 ERROR unit.shard-one-new/1.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-shard-one-new-0: 02:21:26 ERROR unit.shard-one-new/0.juju-log Unable to set params: ['vm.max_map_count']
unit-shard-one-new-0: 02:21:26 ERROR unit.shard-one-new/0.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-shard-one-new-1: 02:21:31 ERROR unit.shard-one-new/1.juju-log This operation (update_relation_data()) can only be performed by the leader unit
unit-shard-two-new-0: 02:21:36 ERROR unit.shard-two-new/0.juju-log Unable to set params: ['vm.max_map_count']
unit-shard-two-new-0: 02:21:36 ERROR unit.shard-two-new/0.juju-log Error setting values on sysctl: Unable to set params: ['vm.max_map_count']
unit-config-server-new-1: 02:22:33 ERROR unit.config-server-new/1.juju-log config-server:18: cmd failed - cmd=['/snap/bin/charmed-mongodb.pbm', 'status', '-o', 'json'], stdout={"Error":"get status of pitr: unable check PITR config status: get config: missed config"}
, stderr=cmd_run.go:1276: WARNING: cannot create user data directory: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH
cmd_run.go:1281: WARNING: cannot copy user Xauthority file: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH

unit-config-server-new-1: 02:22:33 ERROR unit.config-server-new/1.juju-log config-server:18: Failed to get pbm status: cmd failed (1) - cmd=['/snap/bin/charmed-mongodb.pbm', 'status', '-o', 'json'], stdout={"Error":"get status of pitr: unable check PITR config status: get config: missed config"}
, stderr=cmd_run.go:1276: WARNING: cannot create user data directory: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH
cmd_run.go:1281: WARNING: cannot copy user Xauthority file: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH

unit-config-server-new-1: 02:22:34 ERROR unit.config-server-new/1.juju-log config-server:18: cmd failed - cmd=['/snap/bin/charmed-mongodb.pbm', 'status', '-o', 'json'], stdout={"Error":"get status of pitr: unable check PITR config status: get config: missed config"}
, stderr=cmd_run.go:1276: WARNING: cannot create user data directory: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH
cmd_run.go:1281: WARNING: cannot copy user Xauthority file: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH

unit-config-server-new-1: 02:22:34 ERROR unit.config-server-new/1.juju-log config-server:18: Failed to get pbm status: cmd failed (1) - cmd=['/snap/bin/charmed-mongodb.pbm', 'status', '-o', 'json'], stdout={"Error":"get status of pitr: unable check PITR config status: get config: missed config"}
, stderr=cmd_run.go:1276: WARNING: cannot create user data directory: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH
cmd_run.go:1281: WARNING: cannot copy user Xauthority file: cannot get the current user: getent could not be executed: exec: "getent": executable file not found in $PATH

unit-shard-one-new-0: 02:22:57 ERROR unit.shard-one-new/0.juju-log sharding:17: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14c009ec705df73cbc771, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.189', 27017) server_type: RSSecondary, rtt: 0.0012770950015692506>, <ServerDescription ('10.189.170.85', 27017) server_type: RSSecondary, rtt: 0.001005753998470027>]>
unit-shard-one-new-0: 02:23:01 ERROR unit.shard-one-new/0.juju-log sharding:17: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14c049ec705df73cbc772, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.189', 27017) server_type: RSSecondary, rtt: 0.0009046630002558231>, <ServerDescription ('10.189.170.85', 27017) server_type: RSSecondary, rtt: 0.0007586380015709437>]>
unit-shard-one-new-0: 02:23:05 ERROR unit.shard-one-new/0.juju-log sharding:17: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14c089ec705df73cbc773, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.189', 27017) server_type: RSSecondary, rtt: 0.0010331249977753032>, <ServerDescription ('10.189.170.85', 27017) server_type: RSSecondary, rtt: 0.00152577399785514>]>
unit-shard-one-new-0: 02:24:22 ERROR unit.shard-one-new/0.juju-log sharding:17: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14c557a697832ceba4506, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.189', 27017) server_type: RSSecondary, rtt: 0.001319293998676585>, <ServerDescription ('10.189.170.85', 27017) server_type: RSSecondary, rtt: 0.0012053399987053126>]>
unit-shard-one-new-0: 02:24:27 ERROR unit.shard-one-new/0.juju-log sharding:17: Unable to get primary: No replica set members match selector "Primary()", Timeout: 1.0s, Topology Description: <TopologyDescription id: 67b14c597a697832ceba4507, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('10.189.170.189', 27017) server_type: RSSecondary, rtt: 0.001401310000801459>, <ServerDescription ('10.189.170.85', 27017) server_type: RSSecondary, rtt: 0.0009010870016936678>]>
unit-config-server-new-1: 02:30:37 ERROR unit.config-server-new/1.juju-log Restore failed: Cannot restore backup, 'remap-pattern' must be set.

[32mINFO    [0m pytest_operator.plugin:plugin.py:991 Forgetting model main...